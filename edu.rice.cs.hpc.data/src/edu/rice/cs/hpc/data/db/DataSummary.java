
package edu.rice.cs.hpc.data.db;
import java.io.IOException;
import java.io.PrintStream;
import java.io.RandomAccessFile;
import java.nio.ByteBuffer;
import java.nio.MappedByteBuffer;
import java.nio.channels.FileChannel;
import java.nio.channels.FileChannel.MapMode;
import java.util.ArrayList;
import java.util.List;
import java.util.Random;

import edu.rice.cs.hpc.data.experiment.metric.MetricValue;
import edu.rice.cs.hpc.data.experiment.metric.MetricValueSparse;

/*********************************************
 * 
 * Class to handle summary.db file generated by hpcprof
 *
 *********************************************/
public class DataSummary extends DataCommon 
{
	// --------------------------------------------------------------------
	// constants
	// --------------------------------------------------------------------
	private final static String HEADER_MAGIC_STR  = "HPCPROF-tmsdb_____";
	private static final int    METRIC_VALUE_SIZE = 8 + 2;
	private static final int    CCT_RECORD_SIZE   = 4 + 8;
	
	// --------------------------------------------------------------------
	// object variable
	// --------------------------------------------------------------------
	
	private RandomAccessFile file;
	private MappedByteBuffer mappedBuffer;
	
	private List<Tuple> tuple;
	
	private long position_profInfo;
	
	
	// --------------------------------------------------------------------
	// Public methods
	// --------------------------------------------------------------------
	
	/***
	 *  <p>Opening for data summary metric file</p>
	 * (non-Javadoc)
	 * @see edu.rice.cs.hpc.data.db.DataCommon#open(java.lang.String)
	 */
	@Override
	public void open(final String filename)
			throws IOException
	{
		super.open(filename);
		file = new RandomAccessFile(filename, "r");
	}
	

	@Override
	/*
	 * (non-Javadoc)
	 * @see edu.rice.cs.hpc.data.db.DataCommon#printInfo(java.io.PrintStream)
	 */
	public void printInfo( PrintStream out)
	{
		super.printInfo(out);
		
		out.println("\n");
		int cct = 1;
		out.format("[%5d] ", cct);
		printMetrics(out, cct);

		// print random metrics
		for (int i=0; i<15; i++)
		{
			Random r = new Random();
			cct  = r.nextInt((int) numItems);
			out.format("[%5d] ", cct);
			printMetrics(out, cct);
		}
	}
	
	/*******
	 * print a list of metrics for a given CCT index
	 * 
	 * @param out : the outpur stream
	 * @param cct : CCT index
	 */
	private void printMetrics(PrintStream out, int cct)
	{
		try {
			List<MetricValueSparse> values = getMetrics(cct);
			for(MetricValueSparse value: values) {
				System.out.print(value.getIndex() + ": " + value.getValue() + " , ");
			}
		} catch (IOException e) {
			e.printStackTrace();
		} finally {
			out.println();
		}
	}

	/***
	 * Retrieve the list of tuple IDs.
	 * @return List of Tuple
	 */
	public List<Tuple> getTuple() {
		return tuple;
	}
	
	
	/**********
	 * Reading a set of metrics from the file for a given CCT 
	 * This method does not support concurrency. The caller is
	 * responsible to handle mutual exclusion.
	 * 
	 * @param cct_id
	 * @return
	 * @throws IOException
	 */
	public List<MetricValueSparse> getMetrics(int cct_id) 
			throws IOException
	{
		// -------------------------------------------
		// read the list of prof info
		// -------------------------------------------

		int index_profInfo = cct_id * ProfInfo.SIZE;
		
		mappedBuffer.rewind();
		mappedBuffer.position(index_profInfo);
		
		ProfInfo info = new ProfInfo();
		
		info.id_tuple_ptr = mappedBuffer.getLong();
		info.metadata_ptr = mappedBuffer.getLong();
		
		mappedBuffer.getLong();
		mappedBuffer.getLong();
		
		info.num_vals = mappedBuffer.getLong();
		info.num_nz_contexts = mappedBuffer.getInt();
		info.offset = mappedBuffer.getLong();
		
		// -------------------------------------------
		// read the cct context
		// -------------------------------------------
		
		long positionCCT = info.offset   + 
				   		   info.num_vals * METRIC_VALUE_SIZE + 
				   		   cct_id        * CCT_RECORD_SIZE;
		
		byte []buffer = new byte[2 * CCT_RECORD_SIZE];
		
		file.seek(positionCCT);
		file.readFully(buffer);
		
		int cct1  = ByteBuffer.wrap(buffer).getInt();
		long idx1 = ByteBuffer.wrap(buffer).getLong(4);
		
		int cct2  = ByteBuffer.wrap(buffer).getInt(12);
		long idx2 = ByteBuffer.wrap(buffer).getLong(16);
		
		assert(cct1 == cct_id && cct2 != cct1);
		
		// -------------------------------------------
		// initialize the metrics
		// -------------------------------------------

		int numMetrics = (int) (idx2-idx1) / METRIC_VALUE_SIZE;
		
		ArrayList<MetricValueSparse> values = new ArrayList<MetricValueSparse>(numMetrics);
		
		// -------------------------------------------
		// read the metrics
		// -------------------------------------------

		file.seek(info.offset + idx1);
		
		buffer = new byte[(int) (idx2-idx1)];
		file.readFully(buffer);
		
		for(int i=0; i<info.num_vals; i++) {
			float value  = ByteBuffer.wrap(buffer, i*10  , 8).getFloat();
			int metricId = ByteBuffer.wrap(buffer, i*10+8, 2).getInt();
			
			MetricValueSparse mvs = new MetricValueSparse(metricId, value);
			values.add(mvs);
		}
		
		return values;
	}
	
	/*
	 * (non-Javadoc)
	 * @see edu.rice.cs.hpc.data.db.DataCommon#dispose()
	 */
	public void dispose() throws IOException
	{
		mappedBuffer.clear();
		file.close();
		super.dispose();
	}
	

	// --------------------------------------------------------------------
	// Protected methods
	// --------------------------------------------------------------------
	
	@Override
	protected boolean isTypeFormatCorrect(long type) {
		return type==1;
	}

	@Override
	protected boolean isFileHeaderCorrect(String header) {
		return header.equals(HEADER_MAGIC_STR);
	}

	@Override
	protected boolean readNextHeader(FileChannel input) 
			throws IOException
	{
		tuple = new ArrayList<DataSummary.Tuple>((int) numItems);
		
		for (int i=0; i<numItems; i++) {
			ByteBuffer buffer = ByteBuffer.allocate(2);
			int numBytes      = input.read(buffer);
			assert (numBytes > 0);

			// -----------------------------------------
			// read the tuple section
			// -----------------------------------------

			Tuple item = new Tuple();
			
			buffer.flip();
			item.length = buffer.getShort();
			
			int lengthTuple = item.length * (2+8);			
			buffer = ByteBuffer.allocate(lengthTuple);

			numBytes = input.read(buffer);
			assert (numBytes > 0);
			
			item.kind  = new short[item.length];
			item.index = new long[item.length];
			
			buffer.rewind();
			
			for (int j=0; j<item.length; j++) {
				item.kind[j]  = buffer.getShort();
				item.index[j] = buffer.getLong();
			}
			tuple.add(item);
		}

		position_profInfo = input.position();
		mappedBuffer = input.map(MapMode.READ_ONLY, position_profInfo, numItems * ProfInfo.SIZE);
		
		return true;
	}

	// --------------------------------------------------------------------
	// Private methods
	// --------------------------------------------------------------------
	
	protected static class Tuple
	{
		public int length;
		short []kind;
		long  []index;
		
		public String toString() {
			String buff = "len: " + length;
			if (kind != null && index != null)
				for(int i=0; i<kind.length; i++) {
					buff += " (" + kind[i] + ", " + index[i] + ")";
				}
			return buff;
		}
	}
	
	protected static class ProfInfo
	{
		/** the size of the record in bytes  */
		public static final int SIZE = 8 + 8 + 8 + 8 + 8 + 4 + 8;
		
		public long id_tuple_ptr;
		public long metadata_ptr;
		public long num_vals;
		public int  num_nz_contexts;
		public long offset;
	}

	/***************************
	 * unit test 
	 * 
	 * @param argv
	 ***************************/
	public static void main(String []argv)
	{
		final String DEFAULT_FILE = "/home/la5/data/sparse/fib/thread_major_sparse.db";
		final String filename;
		if (argv != null && argv.length>0)
			filename = argv[0];
		else
			filename = DEFAULT_FILE;
		
		final DataSummary summary_data = new DataSummary();
		try {
			summary_data.open(filename);			
			summary_data.printInfo(System.out);
			summary_data.dispose();	
			
		} catch (IOException e) {
			// TODO Auto-generated catch block
			e.printStackTrace();
		}
	}
}
